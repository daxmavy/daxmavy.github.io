---
title: "Maths and ethics don't mix nicely: Ethical AI in business, part 2"
collection: takes
type: "takes"
permalink: /takes/2020-07-26-take2
---

### Racial bias in sentencing

Another famous example is the controversy over the COMPAS algorithm for predicting recidivism, used for supporting decision-making in parole and sentencing in the US. ProRepublica announced that ["There’s software used across the country to predict future criminals. And it’s biased against blacks."](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing). In turn, three criminal justice researchers [rebutted the analysis](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjzt_66z-fqAhVPfX0KHbIhAdkQFjABegQICBAB&url=https%3A%2F%2Fwww.uscourts.gov%2Fsites%2Fdefault%2Ffiles%2F80_2_6_0.pdf&usg=AOvVaw23pT_d9yGcxFGF7f6u_otb), finding no bias. It turns out that [both groups were right](https://www.technologyreview.com/2017/06/12/105804/inspecting-algorithms-for-bias/), but just using different metrics:
- ProRepublica compared the false positive rate of black people (chance of being incorrectly given a high chance of reoffence) to the false negative rate of white people (the chance of being incorrectly given a low chance of reoffence)
- Northpointe (the developers of the algorithm) and the researchers considered positive predictive value, a measure which takes into consideration both false positive and false negative, and according to this measure, both groups were approximately equal.

The question is: which should we use? I think that there's a clear social justice problem if black people are incorrectly sent to jail, and white people are incorrectly sent home - so there is validity in the method of ProRepublica.

But, the solution isn't simple. If you just optimise to prevent people being falsely sent to jail, then your algorithm will *never* recommend jail! Furthermore, empirical differences in recidivism rates among white and black Americans mean that applying the same metric to both groups will always result in differences. 

Perhaps a bigger question is: should an algorithm be contributing towards sentencing? I'll consider this below in the fourth issue.


## 2. Discussion of ethics is rooted in the humanities. Now we can measure it.

So the first lesson is that ethical decisions are now explicit. This section is about how they are now quantifiable - and the difficulties with trying to achieve that.

1. concepts don't map nicely
                      1. asdf
    1. asdf
2. conc

The first obvious thing to say is that for a topic such as fairness, there

nothing new here: it's just measurable!


The quantifiable nature of this discrimination - counterfactual tests, simulation results - is both a promising advantage of this space, and a minefield.

also a well-discussed mathematical difficulty - there are 37 different definitions of discrimination, and some are contradictory

even what it means to explain or understand a decision: is an 'explanation' good enough? What standard of explanation: how the whole model works (good luck...), or an approximate explanation of big factors (read: local linear estimation of the model) 

good:
- greater deal of visibility: you can't ask a personal banker what they would have done differently had the person been 5 years older and a women, but you can do that with an algorithm
- not reliant on theories or qualitative studies by themselves

bad:
1. obsession with limited notions of fairness
2. obsession with quantitative notions of fairness - often, missing 'the human element', and shutting out those groups which are entirely unrepresented in the data
3. ability to find discrimination if you want to ('p-hacking of discrimination'): compas example