---
title: "4 under-discussed reasons for why 'ethical AI' is difficult in a business setting"
collection: takes
type: "takes"
permalink: /takes/1998-07-26-take1
---
tl;dr: 1) the ethical decisions are not new; they're just explicit 2) 'measuring' ethics in a quantitative way is fraught 3) organisational design for ethical AI is hard 4) sometimes AI isn't the problem, the product is the problem

The New York Times test: would you be happy to see your name on the NYT next to your justification of a decision that you made? This is a test for business decision-making that my mum passed on to me from her experience in corporate work. It's appealing, foregoing rigorous conceptual definitions of ethics in favour of simply forcing you to reflect on how you would feel about your decisions appearing outside the corporate bubble.

![image](../images/nyt_title.PNG)

However, what do you do when the topic you're talking about is so technical that the editor of the NYT would revile in horror at the thought of trying to break it down for the average Jane, let alone fitting mathematical equations in a broadsheet column? What if making a decision on the topic would amount to making a quantitatively measurable and explicit definition of discrimination?

This article seeks to outline 4 key issues to consider in making ethical AI happen in business (or, in academic-speak, 'operationalising' the concepts of ethical AI), from my experience as a data scientist working in this area.

There has been a great deal of public interest in the ethics of AI, following a number of high-profile scandals that have affected every big-name tech company (amazon hiring, google gorilla, facebook genocide/elections, apple card pay, netflix hyper-targeting)

I'll start with two examples which illustrate many of the different types of difficulties. If you just want to get to the point(s), click [here](#What-is-'bad-discrimination'-in-a-business-context?)

### Gender bias in healthcare
In 2019 [Babylon Health was accused of gender bias](https://www.babylonhealth.com/blog/tech/doctors-sex-and-the-ai-debate) when its AI-driven health guidance app predicted that a woman presenting with chest pain was having a panic attack, but a man with the same symptoms and other characteristics was predicted to have a heart attack. This seems fairly open-and-shut, especially in the context of women dying because of doctors ignoring their pain. But there's actually three factors that make this really complicated.

*Problems with counterfactual fairness*: Women compared to men at that age have twice the risk of mental health issues, and lower risk of heart conditions. In this case, a simple counterfactual comparison, where you change one variable, is perhaps not appropriate. The fact that gender has a meaningful difference in the outcome of an AI's decision isn't always a bad thing!

*Choosing 'the right' accuracy metric*: From the perspective of an ML algorithm, that was likely the correct decision if you use a metric such as accuracy. However, the consequences of inaccuracy is different in different scenarios. For example, consider the difference between:
- telling someone that they're having a heart attack when they're having a panic attack: now they're even more panicky
- telling someone that they're having a panic attack when they're having a heart attack: now they're dying

How do you construct a metric for your ML algorithm which takes all this into consideration? Where do you draw the line between sending everyone with upper body pain off to the hospital, and only doing so if they are face-down on the floor? 

*Algorithmic bias reflecting bias in society*: Thirdly, what if your data itself is compromised? Medical research is famously biased against women, and women have been historically under-represented as subjects of studies. This prompts three questions:
1. There are some obvious cures to this, such as finding and training on representative datasets. But what if it just doesn't exist? Does a company providing health advice have an ethical obligation to develop medical research?
2. How do you disentangle the *additional* bias created by your algorithm, from that present in your data? Where is the line in the sand which determines what sources of bias a company is responsible for?
3. In cases where it is extremely difficulty to eliminate bias, such as when minority populations are barely represented in the data, is it preferable to produce a worse AI, or no AI?

### Racial bias in sentencing

Another famous example is the controversy over the COMPAS algorithm for predicting recidivism, used for supporting decision-making in parole and sentencing in the US. ProRepublica announced that [There’s software used across the country to predict future criminals. And it’s biased against blacks](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing). In turn, three criminal justice researchers [rebutted the analysis](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjzt_66z-fqAhVPfX0KHbIhAdkQFjABegQICBAB&url=https%3A%2F%2Fwww.uscourts.gov%2Fsites%2Fdefault%2Ffiles%2F80_2_6_0.pdf&usg=AOvVaw23pT_d9yGcxFGF7f6u_otb), finding no bias. It turns out that [both groups were right](https://www.technologyreview.com/2017/06/12/105804/inspecting-algorithms-for-bias/), but just using different metrics:
- ProRepublica compared the false positive rate of black people (chance of being incorrectly given a high chance of reoffence) to the false negative rate of white people (the chance of being incorrectly given a low chance of reoffence)
- Northpointe (the developers of the algorithm) and the researchers considered positive predictive value, a measure which takes into consideration both false positive and false negative, and according to this measure, both groups were approximately equal.

The question is: which should we use? I think that there's a clear social justice problem if black people are incorrectly sent to jail, and white people are incorrectly sent home - so there is validity in the method of ProRepublica.

But, the solution isn't simple. If you just optimise to prevent people being falsely sent to jail, then your algorithm will *never* recommend jail! Furthermore, empirical differences in recidivism rates among white and black Americans mean that applying the same metric to both groups will always result in differences. 

Perhaps a bigger question is: should an algorithm be contributing towards sentencing? I'll consider this below in the fourth issue.

## The response

Everyone now wants to get involved in ethical AI. Oodles of research literature are getting cranked out by the machine that is ML academia, now that 'ethics'+'machine learning' = 'publish'. Several research groups have been established: see the links at the bottom of the page.

There are plenty of conceptual difficulties to consider. However, I think the focus of research on methods rather than systems, and maths over humans, is flawed. Ultimately, it is companies, governments, and other organisations that produce AI systems. It's pointless *knowing* that training on balanced datasets leads to more equitable image recognition, if these organisations are not equipped or incentivised to act on that knowledge - especially if training on imbalanced datasets may be easier, and even lead to better accuracy on wealthier customers.

Why am \*I\* talking about this? Good question! At the bottom of this post there is a collection of articles and books which have either informed my view or which I'm aiming to read. These are written by researchers with far more perceptiveness and experience than I could ever hope for. Centering the contributions of a) members of those communities most affected by AI bias, and b) researchers from outside data science are key components to the solution - so please check out those groups and individuals!

Nonetheless, I have been involved in the operationalisation of ethics in machine learning in two capacities at CBA: firstly in AI Labs, contributing towards capability to detect unwanted biases, and now in the Financial Wellbeing Team, investigating the relationship between financial vulnerability and changes in behaviours.

This relatively unique experience, paired with a reasonably interdisciplinary background, has led to some interesting insights over the last year and a half, which I hope to share below.

## 1. People have always discriminated. Now it's explicit.

Firstly, note that the purpose of ML and data science *is* to discriminate, in the sense of making a choice. Algorithms are designed to make decisions based on data: 
- send ad or not
- approve for loan or not
- cat or not

However, discrimination, aside from its literal meaning, has a negative connotation. A decision or action is discriminatory if that decision or action is made in a particular way which negatively and unfairly affects someone - often along axes of oppression such as race and gender.

However, people have been making decisions for a long time. Before AI existed, people sent ads, approved loans, and decided whether someone is a criminal. The system for deciding whether these decisions and actions were discriminatory is huge and complex, including discussions between ordinary people, our judicial and political systems, and social customs.

The first key change that ethical AI has brought is that decisions about what constitutes discrimination are complex and *explicit*, as opposed to the usual discussion, which is complex but *implicit*.

Before, we trusted people (perhaps not wisely) to make complex ethical decisions. There are three characteristics of such human decision-making:
1. The ethical decision is *subconscious*, often relying on a mixture of emotion and background psychological processes, and very rarely based on logical reasoning.
2. The ethical decision is *conservative* in two senses. Firstly, a common justification is 'this is a tricky ethical issue, but everyone else does this'. Secondly, in the presence of uncertainty and complexity, people revert to the standard that they're used to.
3. The ethical decision is made *privately*. This is primarily because it occurs in someone's head, which we do not have access to.

Because this is such a complicated and mysterious process, we have long relied on evidence of outcomes and behaviours to decide whether a decision is disciminatory, often requiring a large amount of evidence to cast a decision made 

Because the details of such an decision is so complicated, for a decision to be regarded as discrimination, often a high burden of evidence has been required. In the face of complete plausible deniability, or even ignorance, of how a person made their decision in their head


Now, when we build AI systems, those decisions, which before were

also a well-discussed mathematical difficulty - there are 37 different definitions of discrimination, and some are contradictory

however, more broadly than the difficulty of picking from the growing crop of definitions, there are big questions to ask:
- Are there valid business reasons that certain groups are 'discriminated against' in the data? What constitutes a valid business reason? Would you be comfortable explaining your reasoning to the newspaper, or to a judge?
- Are you happy to effectively reduce accuracy for privileged groups, just to reduce the accuracy gap?
- Would you be willing to forego profitability beyond that which is required by law?
- Is the 'bad thing to avoid' a result of AI/ML, or of your product?
- Is it acceptable to use an entirely different model for a subpopulation? Would that count as being disciminatory in a different fashion?

more generally, need to actually know why your organisation cares about ethical AI. Is it
- bad press
- a desire to do what's right by consumers
- a desire to specifically address and right issues faced by those facing discrimination
- because it is currently a buzzword and Stanford has some new research group with a fancy logo

is an 'explanation' good enough? What standard of explanation: how the whole model works (good luck...), or an approximate explanation of big factors (read: local linear estimation of the model) 

## 2. Discussion of ethics has usually been rooted in hypotheticals. Now we can measure it.

nothing new here: it's just measurable!


The quantifiable nature of this discrimination - counterfactual tests, simulation results - is both a promising advantage of this space, and a minefield.

good:
- greater deal of visibility: you can't ask a personal banker what they would have done differently had the person been 5 years older and a women, but you can do that with an algorithm
- not reliant on theories or qualitative studies by themselves

bad:
1. obsession with limited notions of fairness
2. obsession with quantitative notions of fairness - often, missing 'the human element', and shutting out those groups which are entirely unrepresented in the data
3. ability to find discrimination if you want to ('p-hacking of discrimination'): compas example



## 3. The hidden difficulty: scaling ethical AI
==========

- lots of the AI ethics literature amounts to designing a regularisation term, which is added to the utility (or loss) function, and which penalises unfairness according to some definition, and given weight in a way chosen to reflect the ideal balance between accuracy and fairness. This does not reflect the reality of any large-scale ML tool - there is often not a single utility function

- Suppose that you make senior people responsible for specifying what constitutes an acceptable business reason for discrimination. Suppose also that there are 100 models in production under the umbrella of a single such 'responsible person'. How can they deal with this huge workload? they can't do it effectively by themselves. certainly, corporations have gotten very good at scaling solutions to complicated problems - but it's not clear 

- different ethical principles butt into one another: if your organisation doesn't ever find out what someone's race is, then it can never find out whether it's racially discriminatory. However, this data is often *not* captured for a) privacy reasons, b) security issues and c) legitimate fears of its misuse 

- from an organisation design perspective, who do you make responsible for ethical AI? 
  - Data scientists often don't have the right tools or experience - but neither do lawyers. 
    - in fact, this is a *really* unusual intersection of skills
  - Just adding another layer of Risk Management is a possibility - but this usually bureaucratic process doesn't fit in with the agile and iterative process of software development.
  - Making everyone responsible for the problem usually means that no-one is
- The main incentive for workers are their KPIs. But how do you define KPIs for algorithmic ethics in a way that is both meaningful and measurable? goodhart's law

- end-to-end problem: good luck analysing the whole pipeline (especially if there are humans, eg customer service reps). Because it is an end-to-end problem, it touches on many different parts of the organisation - usually more parts than can be ordinarily considered in a single project.

- top-down problem: because a) these decisions are consequential and can land you in the news and b) require touching many different parts of the organisation, high-level decision-makers should usually be responsible, or at least highly involved in, that task. [gradient](https://gradientinstitute.org/blog/4/)
    - but they usually aren't technical! what to do...


## 4. When shouldn't we use AI?
===========
tech is not value-neutral - and neither is apathy

Many of the coolest parts of AI are also the bits with the most danger
- Predictive analytics is often more concerned with accuracy than anything else
- Unsupervised techniques leave a frightening amount of decision-making to algorithmic whimsy, and lend themselves nicely to reductionist characterisations of populations
- Deep learning is entirely unpenetrable: it works, and we have not that many real ideas why. When it starts to reflect the biases deeply engrained in our 

This is a topic for another hot take, but I think that there's a growing recognition that the greatest drivers of value in data science are often not the shiny ML tricks themselves, but rather whatever methods are need to improve customer and user outcomes - including considerations of fairness and ethics. 

Nonetheless, as a maths student and data scientist, it is still disappointing to be told 'you can't play with those toys'

So what can data scientists do?
----------
- not work on shit projects / areas:
    - egs: image recognition; surveillance, promoting addictive behaviours ('sticky' products), predictive policing
- make space for voices that are diverse in two sense:
    - groups from minority communities, both as employees, and as users (or used...)
    - different academic communities: human-computer interaction, sociology
- raise their voices at work: tech workers are uniquely positioned!
- especially if they're practitioners, describe the difficulties in implementing ethics research in AI!
- donate: tech is rewarded richly, and giving back should be a consideration for anyone earning a decent wage in a developed nation. link to effective altruism

The false alternative: 'it's too hard' 
===========

- a universal retreat from 'big data': 
    Perhaps people might say, " well, seems like we should just give up on this whole AI thing"
    but then: just return to hidden discrimination.
    just putting a human in charge 

- don't let this turn into another m'fucking culture war pls
- 

Conclusion
==========

Positive signs:
- my experience at CBA
- greater engagement and awareness in community
- as earlier, this is actually an exciting opportunity to bring issues to light that haven't been brought up before the data revolution!
- tech has demonstrated its ability to listen to and address the needs of majority (esp. wealthy) people. This user-centric approach can be used to focus on the voices of minority groups to bring about the required change.

There is a great deal of optimism, energy, and cautious awareness of the complexity of the issues. We can't wait until the philosophers stop arguing about whether utilitarianism or virtue ethics is better - the process of operationalising ethical considerations in machine learning is happening now. 

There are complexities in this process, and I have outlined some of them above:
1. difficulties with coming up with precise definitions
2. complexity of linking ethics to quantitative data
3. scaling ethical AI in an organisation
4. the tension 

Good posts:

[Contradictions between principles and practice](https://ethicalai.ai/2020/07/14/principles-versus-practice/)

[Ali Alkhatib's essay on the problem with ethics researchers bowing to the powers that be](https://ali-alkhatib.com/blog/anthropological-intelligence)



Resources:

articles that I've liked:
that one from mit
alkathib's essay


books:
algorithms of oppression
weapons of math destruction
made by humans


International groups:
black in AI
fast.ai
[AI Now Institute](https://ainowinstitute.org/)
stanford centre


[there](https://www.theguardian.com/technology/2019/apr/04/google-ai-ethics-council-backlash) [are]

Australian groups:
3ai
gradient institute




