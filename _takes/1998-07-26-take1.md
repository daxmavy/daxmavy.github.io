---
title: "4 under-discussed reasons for why 'ethical AI' is difficult in a business setting"
collection: takes
type: "takes"
permalink: /talks/1998-07-26-take1
#venue: "UC San Francisco, Department of Testing"
#date: 2012-03-01
#location: "San Francisco, California"
---

The New York Times test: would you be happy to see your name on the NYT next to your justification of a decision that you made? This is a test for business decision-making that my mum passed on to me from her experience in corporate work. It's appealing, foregoing rigorous conceptual definitions of ethics in favour of simply forcing you to reflect on how you would feel about your decisions appearing outside the corporate bubble.

However, what do you do when the topic you're talking about is so technical that the editor of the NYT would revile in horror at the thought of trying to break it down for the average Jane, let alone fitting mathematical equations in a broadsheet column? What if making a decision on the topic would amount to making a quantitatively measurable and explicit definition of discrimination?

This article seeks to outline 3 key obstructions to making ethical AI happen in business (or, in academic-speak, 'operationalising' the concepts of ethical AI), and suggest possible routes to addressing them, from my experience as a data scientist working in this area.

There has been a great deal of public interest in the ethics of AI, following a number of high-profile scandals that have affected every member of the FAANG tech companies (amazon hiring, google gorilla, facebook genocide/elections, apple card pay, netflix hyper-targeting)

Everyone wants to get involved in ethical AI. Oodles of research literature are getting cranked out by the machine that is ML academia, now that 'ethics'+'machine learning' = 'publish'. However, I think that a large part of this body of work is missing the mark, as I'll explain below. 

Why am I talking about this? Good question! At the bottom of this post here [add hyperlink] there are a bunch of articles and books which have informed my view, written by researchers with far more perceptiveness and experience than I could ever hope for.

Nonetheless, I have been 'at the coalface' in the operationalisation of ethics in machine learning in two capacities at CBA: firstly in the AI Labs, contributing towards an internal package to detect unwanted biases, and now in the Financial Wellbeing Team, investigating the relationship between financial vulnerability and changes in habits due to COVID-19.

This relatively unique experience, paired with a reasonably interdisciplinary background, has led to some interesting insights over the last year and a half, which I hope to share below.

Some important notes on the limitation of my scope and knowledge: 
- i am but a poor fool
- all of this has been said before probably, apologies
- mostly with an eye to problems with operationalising ethical AI in a large-scale business context 

It's very difficult to be precise about what constitutes 'bad discrimination' in AI in a business context
==========
firstly, note that the purpose of ML and data science *is* to discriminate, in the sense that these algorithms are designed to make decisions based on data: send ad or not, approve for loan or not, cat or not.

also a well-discussed mathematical difficulty - there are 37 different definitions of discrimination, and some are contradictory

however, more broadly than the difficulty of picking from the growing crop of definitions, there are big questions to ask:
- Are there valid business reasons that certain groups are 'discriminated against' in the data? What constitutes a valid business reason? Would you be comfortable explaining your reasoning to the newspaper, or to a judge?
- Are you happy to effectively reduce accuracy for privileged groups, just to reduce the accuracy gap?
- Would you be willing to forego profitability beyond that which is required by law?
- Is the 'bad thing to avoid' a result of AI/ML, or of your product?
- Is it acceptable to use an entirely different model for a subpopulation? Would that count as being disciminatory in a different fashion?

more generally, need to actually know why your organisation cares about ethical AI. Is it
- bad press
- a desire to do what's right by consumers
- a desire to specifically address and right issues faced by those facing discrimination
- because it is currently a buzzword and Stanford has some new research group with a fancy logo

is an 'explanation' good enough? What standard of explanation: how the whole model works (good luck...), or an approximate explanation of big factors (read: local linear estimation of the model) 

Quantitative data and ethics don't mesh nicely
==========
The quantifiable nature of this discrimination - counterfactual tests, simulation results - is both a promising advantage of this space, and a minefield.

good:
- greater deal of visibility: you can't ask a personal banker what they would have done differently had the person been 5 years older and a women, but you can do that with an algorithm
- not reliant on theories or qualitative studies by themselves

bad:
1. obsession with limited notions of fairness
2. obsession with quantitative notions of fairness - often, missing 'the human element', and shutting out those groups which are entirely unrepresented in the data
3. ability to find discrimination if you want to ('p-hacking of discrimination'): compas example

The hidden difficulty: scaling ethical AI
==========

- lots of the AI ethics literature amounts to designing a regularisation term, which is added to the utility (or loss) function, and which penalises unfairness according to some definition, and given weight in a way chosen to reflect the ideal balance between accuracy and fairness. This does not reflect the reality of any large-scale ML tool - there is often not a single utility function

- Suppose that you make senior people responsible for specifying what constitutes an acceptable business reason for discrimination. Suppose also that there are 100 models in production under the umbrella of a single such 'responsible person'. How can they deal with this huge workload? they can't do it effectively by themselves. certainly, corporations have gotten very good at scaling solutions to complicated problems - but it's not clear 

- different ethical principles butt into one another: if your organisation doesn't ever find out what someone's race is, then it can never find out whether it's racially discriminatory. However, this data is often *not* captured for a) privacy reasons, b) security issues and c) legitimate fears of its misuse 

- from an organisation design perspective, who do you make responsible for ethical AI? 
  - Data scientists often don't have the right tools or experience - but neither do lawyers. 
    - in fact, this is a *really* unusual intersection of skills
  - Just adding another layer of Risk Management is a possibility - but this usually bureaucratic process doesn't fit in with the agile and iterative process of software development.
  - Making everyone responsible for the problem usually means that no-one is
- The main incentive for workers are their KPIs. But how do you define KPIs for algorithmic ethics in a way that is both meaningful and measurable? goodhart's law

- end-to-end problem: good luck analysing the whole pipeline (especially if there are humans, eg customer service reps). Because it is an end-to-end problem, it touches on many different parts of the organisation - usually more parts than can be ordinarily considered in a single project.

- top-down problem: because a) these decisions are consequential and can land you in the news and b) require touching many different parts of the organisation, high-level decision-makers should usually be responsible, or at least highly involved in, that task. 
    - but they usually aren't technical! what to do...


The tension between doing what's right, and doing what's cool
===========
tech is not value-neutral - and neither is apathy

Many of the coolest parts of AI are also the bits with the most danger
- Predictive analytics is often more concerned with accuracy than anything else
- Unsupervised techniques leave a frightening amount of decision-making to algorithmic whimsy, and lend themselves nicely to reductionist characterisations of populations
- Deep learning is entirely unpenetrable: it works, and we have not that many real ideas why. When it starts to reflect the biases deeply engrained in our 

This is a topic for another hot take, but I think that there's a growing recognition that the greatest drivers of value in data science are often not the shiny ML tricks themselves, but rather whatever methods are need to improve customer and user outcomes - including considerations of fairness and ethics. 

Nonetheless, as a maths student and data scientist, it is still disappointing to be told 'you can't play with those toys'

So what can data scientists do?
----------
- not work on shit projects / areas:
    - egs: image recognition; surveillance, promoting addictive behaviours ('sticky' products), predictive policing
- make space for voices that are diverse in two sense:
    - groups from minority communities, both as employees, and as users (or used...)
    - different academic communities: human-computer interaction, sociology
- raise their voices at work: tech workers are uniquely positioned!
- especially if they're practitioners, describe the difficulties in implementing ethics research in AI!
- donate: tech is rewarded richly, and giving back should be a consideration for anyone earning a decent wage in a developed nation. link to effective altruism

The false alternative: 'it's too hard' 
===========

- a universal retreat from 'big data': 
    Perhaps people might say, " well, seems like we should just give up on this whole AI thing"
    but then: just return to hidden discrimination.
    just putting a human in charge 

- don't let this turn into another m'fucking culture war pls
- 

Conclusion
==========

Positive signs:
- my experience at CBA
- greater engagement and awareness in community
- as earlier, this is actually an exciting opportunity to bring issues to light that haven't been brought up before the data revolution!
- tech has demonstrated its ability to listen to and address the needs of majority (esp. wealthy) people. This user-centric approach can be used to focus on the voices of minority groups to bring about the required change.

There is a great deal of optimism, energy, and cautious awareness of the complexity of the issues. We can't wait until the philosophers stop arguing about whether utilitarianism or virtue ethics is better - the process of operationalising ethical considerations in machine learning is happening now. 

There are complexities in this process, and I have outlined some of them above:
1. difficulties with coming up with precise definitions
2. complexity of linking ethics to quantitative data
3. scaling ethical AI in an organisation
4. the tension 




Resources:

articles that I've liked:
that one from mit
alkathib's essay


books:
algorithms of oppression
weapons of math destruction
made by humans


International groups:
black in AI
fast.ai
nyu
stanford centre

Australian groups:
3ai
gradient institute




