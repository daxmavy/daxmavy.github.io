---
title: "AI forces us to make ethical decisions explicitly: Ethical AI in business, part 1"
date: 1998-07-26
draft: true
---

This article is the first in a series examining ethical AI implementation in business contexts. The subsequent parts address how mathematical approaches complicate ethical frameworks, scaling challenges in ethical discussions, and instances where product design—rather than AI itself—constitutes the core problem.

## The New York Times Test and Technical Complexity

The author presents the "New York Times test," a decision-making framework asking whether one would accept seeing their reasoning published in the newspaper. While useful for reflection, this approach falters when addressing highly technical AI topics that resist public explanation or when quantifying discrimination becomes necessary.

## High-Profile AI Failures

Recent scandals involving major technology firms—Amazon's hiring bias, Google's image recognition errors, Facebook's content moderation failures, Apple Card disparities, and Netflix's targeting practices—have generated public interest in ethical AI governance. However, the author argues that academic focus on methodologies over systemic implementation remains problematic.

## The Babylon Health Case Study

A 2019 incident involved Babylon Health's app misclassifying a woman's chest pain as panic disorder while correctly identifying identical symptoms in a man as cardiac arrest. Three complicating factors emerged:

**Counterfactual fairness limitations**: Gender differences in mental health and cardiac risk rates mean simple variable comparisons prove inappropriate.

**Accuracy metrics**: Different misdiagnosis consequences demand careful metric selection—confusing panic attacks with heart attacks carries vastly different stakes.

**Historical data bias**: Medical research historically underrepresents female subjects, potentially compromising algorithmic training data.

## Making Discrimination Explicit

The author emphasizes a fundamental shift: "machine learning pushes us to define fairness" by requiring explicit rather than implicit ethical reasoning. Previously, human decision-making remained largely private and subconscious, leaving evaluation dependent on secondary evidence like behavior, justifications, or stated intentions.

AI systems invert this dynamic. Unlike recruiter bias (hidden internally), algorithmic decisions become measurable across populations, revealing empirical discrimination patterns. This explicitness eliminates plausible deniability.

## Organizational Implications

Companies now face concrete decisions:

- Determining valid business justifications for group-based disparities
- Calculating acceptable costs for bias reduction beyond legal requirements
- Deciding demographic-specific model approaches
- Establishing acceptable bias thresholds before market launch

These determinations require connecting explicit technical choices to organizational purpose—a challenging shift for institutions historically relying on implicit standards.

## Opportunities From Transparency

Three benefits emerge from forcing explicit ethical discussions:

**Quantified tradeoffs**: Previously abstract equity tensions now become measurable, enabling substantive dialogue about fairness competing values.

**Accountability beyond intentions**: Structural biases cannot be excused through good intentions when outcomes prove quantifiable.

**Evidence-based evaluation**: Moving from post-hoc justifications toward observable decision patterns represents intellectual progress in addressing bias.

## Conclusion

AI's requirement for explicit ethical operationalization creates institutional pressure to move beyond vague aspirational principles toward concrete policy. While Australian frameworks like the government's Ethical AI Principles provide starting points, their voluntary nature leaves significant gaps. The challenge lies in developing institutional capacity to make these explicit choices without retreating into established but potentially biased defaults.
